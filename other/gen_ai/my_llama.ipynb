{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webapp/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama.cpp: loading model from /home/webapp/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN THE MODEL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "from googletrans import Translator\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from TTS.api import TTS\n",
    "\n",
    "\"\"\"Text-Generation > story_info\"\"\"\n",
    "\n",
    "def text_generation(title):\n",
    "    split_words = ['paragraph 1:', 'illustration 1:', 'paragraph 2:', 'illustration 2:', 'paragraph 3:', 'illustration 3:', 'paragraph 4:', 'illustration 4:']\n",
    "    model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "    model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
    "    model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "    lcpp_llm = Llama(\n",
    "        model_path=model_path,\n",
    "        n_threads=2,\n",
    "        n_batch=512,\n",
    "        n_gpu_layers=32\n",
    "    )\n",
    "    lcpp_llm.params.n_gpu_layers\n",
    "    prompt = f'''\n",
    "    Taking Andersen's story as an example, write a 'very short' picture book story. It should includes four paragraphs (no more than 30 words) and illustration description in each paragraph.\n",
    "    Title: {title}.\n",
    "    Format please follows:\n",
    "        Title:\n",
    "        Paragraph 1:\n",
    "        Illustration 1:\n",
    "        Paragraph 2:\n",
    "        Illustration 2:\n",
    "        Paragraph 3:\n",
    "        Illustration 3:\n",
    "        Paragraph 4:\n",
    "        Illustration 4:\n",
    "    '''\n",
    "    while True:\n",
    "        print('RUN THE MODEL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        response = lcpp_llm(prompt=prompt, max_tokens=1200, temperature=0.5, top_p=0.95, repeat_penalty=1.2, top_k=150, echo=True)\n",
    "        full_story = response[\"choices\"][0][\"text\"]\n",
    "        if all(full_story.lower().count(word.lower()) == 2 for word in split_words):\n",
    "            print(full_story)\n",
    "            return full_story\n",
    "        \n",
    "title = '小紅帽'\n",
    "full_story = text_generation(title)\n",
    "print(full_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ee1d0d1a77411a870a914a5d654252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webapp/AI_PictureBooks_Web/genai/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/webapp/AI_PictureBooks_Web/genai/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/webapp/AI_PictureBooks_Web/genai/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/webapp/AI_PictureBooks_Web/genai/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Note: The story should be no more than 100 words.\n",
      "\n",
      "Title: Little cat and dog\n",
      "\n",
      "Paragraph 1:\n",
      "Little cat and dog were the best of friends.\n",
      "\n",
      "Illustration 1: A picture of a cat and dog playing together.\n",
      "\n",
      "Paragraph 2:\n",
      "They would play chase and cuddle, and have so much fun.\n",
      "\n",
      "Illustration 2: A picture of the cat and dog running and jumping.\n",
      "\n",
      "Paragraph 3:\n",
      "One day, they found a ball and played with it all day.\n",
      "\n",
      "Illustration 3: A picture of the cat and dog playing with a ball.\n",
      "\n",
      "Paragraph 4:\n",
      "They were so happy, and their friendship grew even stronger.\n",
      "\n",
      "Illustration 4: A picture of the cat and dog sitting together, smiling.\n",
      "\n",
      "Note: The story is very short, only 4 paragraphs, and each paragraph is around 20-25 words. The illustrations are simple and easy to understand.\n"
     ]
    }
   ],
   "source": [
    "# 測試版(加速版)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# from peft import LoraConfig\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Model from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# New instruction dataset\n",
    "guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = \"llama-2-7b-chat-guanaco\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def text_generation(title):\n",
    "    prompt = f'''\n",
    "        Taking Andersen's story as an example, write a 'very short' picture book story in English. \n",
    "        Title: {title}.\n",
    "        It should includes four paragraphs, please follow the output format:\n",
    "            Title:\n",
    "            Paragraph 1:\n",
    "            Illustration 1:\n",
    "            Paragraph 2:\n",
    "            Illustration 2:\n",
    "            Paragraph 3:\n",
    "            Illustration 3:\n",
    "            Paragraph 4:\n",
    "            Illustration 4:\n",
    "        '''\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=700,truncation=True)\n",
    "    result = pipe(prompt)\n",
    "    full_story = result[0]['generated_text']\n",
    "    full_story = full_story.replace(prompt,'')\n",
    "    \n",
    "    clear_memory()  # Clear memory after generation\n",
    "    return full_story\n",
    "\n",
    "title = 'Little cat and dog'\n",
    "full_story = text_generation(title)\n",
    "print(full_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
