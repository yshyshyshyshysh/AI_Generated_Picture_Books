{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 慢版\n",
    "\n",
    "# import re\n",
    "# import requests\n",
    "# from huggingface_hub import hf_hub_download\n",
    "# from llama_cpp import Llama\n",
    "# from googletrans import Translator\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# import io\n",
    "# from TTS.api import TTS\n",
    "\n",
    "# \"\"\"Text-Generation > story_info\"\"\"\n",
    "\n",
    "# def text_generation(title):\n",
    "#     split_words = ['paragraph 1:', 'illustration 1:', 'paragraph 2:', 'illustration 2:', 'paragraph 3:', 'illustration 3:', 'paragraph 4:', 'illustration 4:']\n",
    "#     model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "#     model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
    "#     model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "#     lcpp_llm = Llama(\n",
    "#         model_path=model_path,\n",
    "#         n_threads=2,\n",
    "#         n_batch=512,\n",
    "#         n_gpu_layers=32\n",
    "#     )\n",
    "#     lcpp_llm.params.n_gpu_layers\n",
    "#     prompt = f'''\n",
    "#     Taking Andersen's story as an example, write a 'very short' picture book story. It should includes four paragraphs (no more than 30 words) and illustration description in each paragraph.\n",
    "#     Title: {title}.\n",
    "#     Format please follows:\n",
    "#         Title:\n",
    "#         Paragraph 1:\n",
    "#         Illustration 1:\n",
    "#         Paragraph 2:\n",
    "#         Illustration 2:\n",
    "#         Paragraph 3:\n",
    "#         Illustration 3:\n",
    "#         Paragraph 4:\n",
    "#         Illustration 4:\n",
    "#     '''\n",
    "#     while True:\n",
    "#         print('RUN THE MODEL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "#         response = lcpp_llm(prompt=prompt, max_tokens=1200, temperature=0.5, top_p=0.95, repeat_penalty=1.2, top_k=150, echo=True)\n",
    "#         full_story = response[\"choices\"][0][\"text\"]\n",
    "#         if all(full_story.lower().count(word.lower()) == 2 for word in split_words):\n",
    "#             print(full_story)\n",
    "#             return full_story\n",
    "        \n",
    "# title = '小紅帽'\n",
    "# full_story = text_generation(title)\n",
    "# print(full_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webapp/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      "/home/webapp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/webapp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/webapp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/webapp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 測試版(加速版)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# from peft import LoraConfig\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Model from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# New instruction dataset\n",
    "guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = \"llama-2-7b-chat-guanaco\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def text_generation(title):\n",
    "    prompt = f'''\n",
    "        Taking Andersen's story as an example, write a 'very short' picture book story in English. \n",
    "        Title: {title}.\n",
    "        It should includes four paragraphs, please follow the output format:\n",
    "            Title:\n",
    "            Paragraph 1:\n",
    "            Illustration 1:\n",
    "            Paragraph 2:\n",
    "            Illustration 2:\n",
    "            Paragraph 3:\n",
    "            Illustration 3:\n",
    "            Paragraph 4:\n",
    "            Illustration 4:\n",
    "        '''\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=700,truncation=True)\n",
    "    result = pipe(prompt)\n",
    "    full_story = result[0]['generated_text']\n",
    "    full_story = full_story.replace(prompt,'')\n",
    "    \n",
    "    clear_memory()  # Clear memory after generation\n",
    "    return full_story\n",
    "\n",
    "def extract_story_info(text):\n",
    "    import re\n",
    "    split_words = ['paragraph 1:', 'illustration 1:', 'paragraph 2:', 'illustration 2:', 'paragraph 3:', 'illustration 3:', 'paragraph 4:', 'illustration 4:']\n",
    "    keys = ['paragraph 1', 'illustration 1', 'paragraph 2', 'illustration 2', 'paragraph 3', 'illustration 3', 'paragraph 4', 'illustration 4']\n",
    "\n",
    "    # Split text into required paragrapgs and illustrations\n",
    "    pattern = '|'.join(map(re.escape, split_words)) # Construct a regular expression (正則表達式) pattern to match multiple keywords and ignore case (大小寫)\n",
    "    pattern = f'(?i)({pattern})'\n",
    "    split_text = re.split(pattern, text) # Split using regular expressions\n",
    "    split_text = [chunk.strip() for chunk in split_text if chunk.strip()] # Remove empty strings\n",
    "\n",
    "    # Build dictionary\n",
    "    story_info = {}\n",
    "    for i in range(len(keys)):\n",
    "        key = keys[i]\n",
    "        value = split_text[i * 2 + 11] # 有...就要用18\n",
    "        story_info[key] = value\n",
    "\n",
    "    # Delete some ending sentence like 'I hope this helps you to write a delightful story for young readers!' in the end\n",
    "    if '\\n' in story_info['illustration 4']:\n",
    "        split_illustration = story_info['illustration 4'].split('\\n')\n",
    "        story_info['illustration 4'] = '\\n'.join(split_illustration[:-1])\n",
    "\n",
    "    return story_info\n",
    "\n",
    "def translate_to_eng(title):\n",
    "    # !pip install googletrans==4.0.0rc1\n",
    "    from googletrans import Translator, LANGUAGES\n",
    "\n",
    "    # Generate translation\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(title, dest='en')\n",
    "    return translation.text\n",
    "\n",
    "def generate_story(title):\n",
    "    title = translate_to_eng(title)\n",
    "    full_story = text_generation(title)\n",
    "    story_info = extract_story_info(full_story)\n",
    "    story_info['title'] = title\n",
    "    return story_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Note: The story should be no more than 100 words.\n",
      "\n",
      "Title: Little cat and dog\n",
      "\n",
      "Paragraph 1:\n",
      "Little cat and dog were the best of friends.\n",
      "\n",
      "Illustration 1: A picture of a cat and dog playing together.\n",
      "\n",
      "Paragraph 2:\n",
      "They would play chase and cuddle, and have so much fun.\n",
      "\n",
      "Illustration 2: A picture of the cat and dog running and jumping.\n",
      "\n",
      "Paragraph 3:\n",
      "One day, they found a ball and played with it all day.\n",
      "\n",
      "Illustration 3: A picture of the cat and dog playing with a ball.\n",
      "\n",
      "Paragraph 4:\n",
      "They were so happy, and their friendship grew even stronger.\n",
      "\n",
      "Illustration 4: A picture of the cat and dog sitting together, smiling.\n",
      "\n",
      "Note: The story is very short, only 4 paragraphs, and each paragraph is around 20-25 words. The illustrations are simple and easy to understand.\n"
     ]
    }
   ],
   "source": [
    "title = 'Little cat and dog'\n",
    "full_story = text_generation(title)\n",
    "print(full_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webapp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/webapp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Note: The story should be no more than 100 words.\n",
      "\n",
      "Title: Little cat and dog\n",
      "\n",
      "Paragraph 1:\n",
      "Little cat and dog were the best of friends.\n",
      "They lived in a cozy little house together.\n",
      "\n",
      "Illustration 1: An illustration of a cat and dog sitting on a couch together, looking happy.\n",
      "\n",
      "Paragraph 2:\n",
      "One day, they decided to go on an adventure.\n",
      "They walked through the forest, and climbed a tree.\n",
      "\n",
      "Illustration 2: An illustration of a cat and dog climbing a tree, with a bird's nest in the branches.\n",
      "\n",
      "Paragraph 3:\n",
      "But then, they heard a loud growl.\n",
      "They looked down and saw a big bear coming towards them.\n",
      "\n",
      "Illustration 3: An illustration of a bear charging towards the cat and dog.\n",
      "\n",
      "Paragraph 4:\n",
      "Little cat and dog were scared, but they didn't want to run away.\n",
      "So they stood up straight and barked loudly.\n",
      "\n",
      "Illustration 4: An illustration of the cat and dog standing up straight and barking at the bear.\n",
      "\n",
      "The end.\n"
     ]
    }
   ],
   "source": [
    "prompt = f'''\n",
    "    Taking Andersen's story as an example, write a 'very short' picture book story in English. \n",
    "    Title: {title}.\n",
    "    It should includes four paragraphs, please follow the output format:\n",
    "        Title:\n",
    "        Paragraph 1:\n",
    "        Illustration 1:\n",
    "        Paragraph 2:\n",
    "        Illustration 2:\n",
    "        Paragraph 3:\n",
    "        Illustration 3:\n",
    "        Paragraph 4:\n",
    "        Illustration 4:\n",
    "    '''\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=700,truncation=True)\n",
    "result = pipe(prompt)\n",
    "full_story = result[0]['generated_text']\n",
    "full_story = full_story.replace(prompt,'')\n",
    "\n",
    "clear_memory()  # Clear memory after generation\n",
    "\n",
    "print(full_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m story_info \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_story\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Show results\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m story_info\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[1], line 115\u001b[0m, in \u001b[0;36mgenerate_story\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m    113\u001b[0m title \u001b[38;5;241m=\u001b[39m translate_to_eng(title)\n\u001b[1;32m    114\u001b[0m full_story \u001b[38;5;241m=\u001b[39m text_generation(title)\n\u001b[0;32m--> 115\u001b[0m story_info \u001b[38;5;241m=\u001b[39m \u001b[43mextract_story_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_story\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m story_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m title\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m story_info\n",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m, in \u001b[0;36mextract_story_info\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(keys)):\n\u001b[1;32m     92\u001b[0m     key \u001b[38;5;241m=\u001b[39m keys[i]\n\u001b[0;32m---> 93\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43msplit_text\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# 有...就要用18\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     story_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Delete some ending sentence like 'I hope this helps you to write a delightful story for young readers!' in the end\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "story_info = generate_story(title)\n",
    "\n",
    "# Show results\n",
    "for key, value in story_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    print(\"=================================\")\n",
    "print(f'story_info = {story_info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/webapp/genai/bin/pip: /home/webapp/AI_PictureBooks_Web/genai/bin/python3: bad interpreter: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip show googletrans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
