{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 慢版\n",
    "\n",
    "# import re\n",
    "# import requests\n",
    "# from huggingface_hub import hf_hub_download\n",
    "# from llama_cpp import Llama\n",
    "# from googletrans import Translator\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# import io\n",
    "# from TTS.api import TTS\n",
    "\n",
    "# \"\"\"Text-Generation > story_info\"\"\"\n",
    "\n",
    "# def text_generation(title):\n",
    "#     split_words = ['paragraph 1:', 'illustration 1:', 'paragraph 2:', 'illustration 2:', 'paragraph 3:', 'illustration 3:', 'paragraph 4:', 'illustration 4:']\n",
    "#     model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "#     model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
    "#     model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "#     lcpp_llm = Llama(\n",
    "#         model_path=model_path,\n",
    "#         n_threads=2,\n",
    "#         n_batch=512,\n",
    "#         n_gpu_layers=32\n",
    "#     )\n",
    "#     lcpp_llm.params.n_gpu_layers\n",
    "#     prompt = f'''\n",
    "#     Taking Andersen's story as an example, write a 'very short' picture book story. It should includes four paragraphs (no more than 30 words) and illustration description in each paragraph.\n",
    "#     Title: {title}.\n",
    "#     Format please follows:\n",
    "#         Title:\n",
    "#         Paragraph 1:\n",
    "#         Illustration 1:\n",
    "#         Paragraph 2:\n",
    "#         Illustration 2:\n",
    "#         Paragraph 3:\n",
    "#         Illustration 3:\n",
    "#         Paragraph 4:\n",
    "#         Illustration 4:\n",
    "#     '''\n",
    "#     while True:\n",
    "#         print('RUN THE MODEL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "#         response = lcpp_llm(prompt=prompt, max_tokens=1200, temperature=0.5, top_p=0.95, repeat_penalty=1.2, top_k=150, echo=True)\n",
    "#         full_story = response[\"choices\"][0][\"text\"]\n",
    "#         if all(full_story.lower().count(word.lower()) == 2 for word in split_words):\n",
    "#             print(full_story)\n",
    "#             return full_story\n",
    "        \n",
    "# title = '小紅帽'\n",
    "# full_story = text_generation(title)\n",
    "# print(full_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c36fa7a78f54d0385e05b975f02262f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 測試版(加速版)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# from peft import LoraConfig\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Model from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# New instruction dataset\n",
    "guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = \"llama-2-7b-chat-guanaco\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def text_generation(title):\n",
    "    prompt = f'''\n",
    "        Taking Andersen's story as an example, write a 'very short' picture book story in English. \n",
    "        Title: {title}.\n",
    "        It should includes four paragraphs, please follow the output format:\n",
    "            Title:\n",
    "            Paragraph 1:\n",
    "            Illustration 1:\n",
    "            Paragraph 2:\n",
    "            Illustration 2:\n",
    "            Paragraph 3:\n",
    "            Illustration 3:\n",
    "            Paragraph 4:\n",
    "            Illustration 4:\n",
    "        '''\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=700,truncation=True)\n",
    "    result = pipe(prompt)\n",
    "    full_story = result[0]['generated_text']\n",
    "    full_story = full_story.replace(prompt,'')\n",
    "    \n",
    "    clear_memory()  # Clear memory after generation\n",
    "    return full_story\n",
    "\n",
    "def extract_story_info(text):\n",
    "    import re\n",
    "    split_words = ['paragraph 1:', 'illustration 1:', 'paragraph 2:', 'illustration 2:', 'paragraph 3:', 'illustration 3:', 'paragraph 4:', 'illustration 4:']\n",
    "    keys = ['paragraph 1', 'illustration 1', 'paragraph 2', 'illustration 2', 'paragraph 3', 'illustration 3', 'paragraph 4', 'illustration 4']\n",
    "\n",
    "    # Split text into required paragrapgs and illustrations\n",
    "    pattern = '|'.join(map(re.escape, split_words)) # Construct a regular expression (正則表達式) pattern to match multiple keywords and ignore case (大小寫)\n",
    "    pattern = f'(?i)({pattern})'\n",
    "    split_text = re.split(pattern, text) # Split using regular expressions\n",
    "    split_text = [chunk.strip() for chunk in split_text if chunk.strip()] # Remove empty strings\n",
    "\n",
    "    # Build dictionary\n",
    "    story_info = {}\n",
    "    for i in range(len(keys)):\n",
    "        key = keys[i]\n",
    "        value = split_text[i * 2 + 11] # 有...就要用18\n",
    "        story_info[key] = value\n",
    "\n",
    "    # Delete some ending sentence like 'I hope this helps you to write a delightful story for young readers!' in the end\n",
    "    if '\\n' in story_info['illustration 4']:\n",
    "        split_illustration = story_info['illustration 4'].split('\\n')\n",
    "        story_info['illustration 4'] = '\\n'.join(split_illustration[:-1])\n",
    "\n",
    "    return story_info\n",
    "\n",
    "def translate_to_eng(title):\n",
    "    # !pip install googletrans==4.0.0rc1\n",
    "    from googletrans import Translator, LANGUAGES\n",
    "\n",
    "    # Generate translation\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(title, dest='en')\n",
    "    return translation.text\n",
    "\n",
    "def generate_story(title):\n",
    "    title = translate_to_eng(title)\n",
    "    full_story = text_generation(title)\n",
    "    story_info = extract_story_info(full_story)\n",
    "    story_info['title'] = title\n",
    "    return story_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Note: The story should be no more than 100 words.\n",
      "\n",
      "Title: Little cat and dog\n",
      "\n",
      "Paragraph 1:\n",
      "Little cat and dog were the best of friends.\n",
      "\n",
      "Illustration 1: A picture of a cat and dog playing together.\n",
      "\n",
      "Paragraph 2:\n",
      "They would play chase and cuddle, and have so much fun.\n",
      "\n",
      "Illustration 2: A picture of the cat and dog running and jumping.\n",
      "\n",
      "Paragraph 3:\n",
      "One day, they found a ball and played with it all day.\n",
      "\n",
      "Illustration 3: A picture of the cat and dog playing with a ball.\n",
      "\n",
      "Paragraph 4:\n",
      "They were so happy, and their friendship grew even stronger.\n",
      "\n",
      "Illustration 4: A picture of the cat and dog sitting together, smiling.\n",
      "\n",
      "Note: The story is very short, only 4 paragraphs, and each paragraph is around 20-25 words. The illustrations are simple and easy to understand.\n"
     ]
    }
   ],
   "source": [
    "title = 'Little cat and dog'\n",
    "full_story = text_generation(title)\n",
    "print(full_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googletrans'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m story_info \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_story\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Show results\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m story_info\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[5], line 113\u001b[0m, in \u001b[0;36mgenerate_story\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_story\u001b[39m(title):\n\u001b[0;32m--> 113\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_to_eng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     full_story \u001b[38;5;241m=\u001b[39m text_generation(title)\n\u001b[1;32m    115\u001b[0m     story_info \u001b[38;5;241m=\u001b[39m extract_story_info(full_story)\n",
      "Cell \u001b[0;32mIn[5], line 105\u001b[0m, in \u001b[0;36mtranslate_to_eng\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_to_eng\u001b[39m(title):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# !pip install googletrans==4.0.0rc1\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator, LANGUAGES\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Generate translation\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     translator \u001b[38;5;241m=\u001b[39m Translator()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'googletrans'"
     ]
    }
   ],
   "source": [
    "story_info = generate_story(title)\n",
    "\n",
    "# Show results\n",
    "for key, value in story_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    print(\"=================================\")\n",
    "print(f'story_info = {story_info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/webapp/genai/bin/pip: /home/webapp/AI_PictureBooks_Web/genai/bin/python3: bad interpreter: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip show googletrans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
